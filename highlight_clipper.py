"""
Highlight Clipping Agent
========================
Finds the peak emotional moment in a raw clip by reading its emotion_report.json,
aligns the cut to the nearest sentence start, and exports a 30-second highlight.

Works with the agentclipz folder layout:
    clips/
      raw_clip_1.mp4
      clip_1/
        emotion_report.json   (utterances with start_ms, duration_ms, emotion, emotion_score)

No API keys required — this is a pure offline video trimmer that uses the
emotion data already generated by the Modulate pipeline in clip_trigger.py.

Usage:
    python highlight_clipper.py                              # process all clips in ./clips
    python highlight_clipper.py clips/clip_1                 # process a specific clip folder
    python highlight_clipper.py clips/clip_1/emotion_report.json   # same, via JSON path
    python highlight_clipper.py clips/clip_1 -o my_highlight.mp4   # custom output path
"""

import json
import logging
import sys
from pathlib import Path

import numpy as np
import pandas as pd
from moviepy.editor import VideoFileClip

logger = logging.getLogger(__name__)

CLIPS_DIR = Path(__file__).resolve().parent / "clips"
CLIP_DURATION_SEC = 30
SENTENCE_WINDOW_BEFORE_PEAK_SEC = 5

# Emotion weights — high-energy emotions score higher for highlight detection
EMOTION_WEIGHTS = {
    "Excited": 1.0, "Angry": 0.95, "Surprised": 0.9, "Afraid": 0.85, "Frustrated": 0.85,
    "Contemptuous": 0.8, "Stressed": 0.8, "Anxious": 0.75, "Disgusted": 0.75, "Amused": 0.7,
    "Happy": 0.6, "Proud": 0.6, "Confident": 0.55, "Concerned": 0.5, "Hopeful": 0.5,
    "Affectionate": 0.45, "Interested": 0.45, "Confused": 0.45, "Disappointed": 0.4,
    "Sad": 0.4, "Ashamed": 0.4, "Relieved": 0.35,
    "Calm": 0.15, "Neutral": 0.1, "Bored": 0.05, "Tired": 0.05,
}


def _resolve_paths(input_path: Path) -> tuple[Path, Path, dict]:
    """
    Resolve the video file and emotion_report.json from a given input path.

    Accepts:
      - A clip folder:  clips/clip_1/
      - A JSON file:    clips/clip_1/emotion_report.json
      - The clips root: clips/  (picks the first clip folder found)

    Returns (video_path, json_path, data).
    """
    input_path = Path(input_path).resolve()

    # Case 1: Direct JSON file
    if input_path.is_file() and input_path.suffix == ".json":
        json_path = input_path
        clip_dir = json_path.parent
    # Case 2: A clip subfolder like clips/clip_1/
    elif input_path.is_dir() and (input_path / "emotion_report.json").exists():
        clip_dir = input_path
        json_path = clip_dir / "emotion_report.json"
    # Case 3: The root clips/ directory — pick the first clip folder
    elif input_path.is_dir():
        clip_dirs = sorted([
            d for d in input_path.iterdir()
            if d.is_dir() and (d / "emotion_report.json").exists()
        ])
        if not clip_dirs:
            raise FileNotFoundError(f"No clip folders with emotion_report.json found in {input_path}")
        clip_dir = clip_dirs[0]
        json_path = clip_dir / "emotion_report.json"
        logger.info("Auto-selected clip folder: %s", clip_dir.name)
    else:
        raise FileNotFoundError(f"Cannot resolve clip from: {input_path}")

    with open(json_path, encoding="utf-8") as f:
        data = json.load(f)

    # Find the matching raw video: raw_clip_N.mp4 lives in clip_dir's parent
    video_name = data.get("video", "")
    video_search_dir = clip_dir.parent  # e.g. clips/

    video_path = video_search_dir / video_name
    if not video_path.exists():
        # Fallback: try to infer from folder name (clip_1 -> raw_clip_1.mp4)
        folder_name = clip_dir.name  # e.g. "clip_1"
        for ext in (".mp4", ".webm", ".mov"):
            candidate = video_search_dir / f"raw_{folder_name}{ext}"
            if candidate.exists():
                video_path = candidate
                break

    if not video_path.exists():
        raise FileNotFoundError(
            f"Video not found. Looked for '{video_name}' in {video_search_dir}. "
            f"Make sure raw_clip_N.mp4 is in the clips/ directory."
        )

    return video_path, json_path, data


def _build_emotion_timeline(data: dict) -> pd.DataFrame:
    """
    Build a per-half-second emotion score timeline from the utterances.
    Each utterance's emotion_score is spread across its duration.
    """
    utterances = data.get("utterances", [])
    if not utterances:
        return pd.DataFrame(columns=["timestamp", "score"])

    # Find the total duration from the last utterance
    last = utterances[-1]
    total_ms = last["start_ms"] + last["duration_ms"]
    total_sec = total_ms / 1000.0

    # Build a 0.5s grid
    grid_times = np.arange(0, total_sec + 0.5, 0.5)
    scores = np.zeros_like(grid_times)

    for u in utterances:
        start_s = u["start_ms"] / 1000.0
        end_s = start_s + u["duration_ms"] / 1000.0
        emotion = u.get("emotion", "Neutral")
        # Use the weight from our table, or the raw score if emotion isn't in the table
        weight = EMOTION_WEIGHTS.get(emotion, u.get("emotion_score", 0.1))

        # Add the weight to every grid cell that overlaps this utterance
        mask = (grid_times >= start_s) & (grid_times <= end_s)
        scores[mask] += weight

    return pd.DataFrame({"timestamp": grid_times, "score": scores})


def _find_t_peak(data: dict) -> float:
    """Find the timestamp where the emotion score is highest."""
    timeline = _build_emotion_timeline(data)
    if timeline.empty:
        return 0.0
    best_idx = timeline["score"].idxmax()
    return float(timeline.loc[best_idx, "timestamp"])


def _find_aligned_start(data: dict, t_peak: float) -> float:
    """Find the nearest sentence/utterance start within 5s before the peak."""
    utterances = data.get("utterances", [])
    window_start = max(0, t_peak - SENTENCE_WINDOW_BEFORE_PEAK_SEC)
    window_end = t_peak

    candidates = [
        u["start_ms"] / 1000.0
        for u in utterances
        if window_start <= u["start_ms"] / 1000.0 <= window_end
    ]

    if not candidates:
        return t_peak
    return max(candidates)  # pick the closest one before the peak


def _compute_clip_bounds(aligned_start: float, source_duration: float) -> tuple[float, float]:
    """Compute start/end for a 30s clip, clamped to [0, source_duration]."""
    clip_start = aligned_start
    clip_end = clip_start + CLIP_DURATION_SEC

    if clip_start < 0:
        clip_start = 0
        clip_end = min(CLIP_DURATION_SEC, source_duration)
    elif clip_end > source_duration:
        clip_end = source_duration
        clip_start = max(0, clip_end - CLIP_DURATION_SEC)

    return clip_start, clip_end


def get_best_highlight(
    input_path: Path | str | None = None,
    output_path: Path | str | None = None,
) -> str:
    """
    Find the best 30-second highlight and export to MP4.

    Args:
        input_path: Path to a clip folder (clips/clip_1), a JSON file
                    (clips/clip_1/emotion_report.json), or the clips root dir.
                    Defaults to ./clips.
        output_path: Where to save the highlight. Defaults to clips/clip_N/highlight.mp4.

    Returns:
        Path to the exported highlight MP4.

    No API keys needed — this runs entirely offline using moviepy.
    """
    input_path = Path(input_path) if input_path else CLIPS_DIR
    video_path, json_path, data = _resolve_paths(input_path)

    clip_dir = json_path.parent  # e.g. clips/clip_1/

    # Get actual video duration
    with VideoFileClip(str(video_path)) as vid:
        source_duration = vid.duration

    # If the video is already <= 30s, just copy it as the highlight
    if source_duration <= CLIP_DURATION_SEC + 1:
        logger.info("Video is already %.0fs (≤%ds), using entire clip as highlight",
                     source_duration, CLIP_DURATION_SEC)
        t_peak = 0.0
        clip_start, clip_end = 0.0, source_duration
    else:
        t_peak = _find_t_peak(data)
        aligned_start = _find_aligned_start(data, t_peak)
        clip_start, clip_end = _compute_clip_bounds(aligned_start, source_duration)

    if output_path is None:
        output_path = clip_dir / "highlight.mp4"
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    logger.info("Peak emotion at %.1fs, cutting [%.1f–%.1fs] from %s",
                t_peak, clip_start, clip_end, video_path.name)

    with VideoFileClip(str(video_path)) as clip:
        subclip = clip.subclip(clip_start, clip_end)
        subclip.write_videofile(str(output_path), codec="libx264", audio_codec="aac", logger=None)

    logger.info("Highlight exported: %s (peak=%.1fs, cut=[%.1f–%.1fs])",
                output_path, t_peak, clip_start, clip_end)
    return str(output_path)
